from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import IsolationForest
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

plt.rcParams['font.sans-serif'] = ['SimHei']  # è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜


screw_order = [1, 2, 4, 41]


def validate_data(row):
    """éªŒè¯æ¯ä¸ªæ ·æœ¬çš„angleå’Œtorqueæ•°æ®é•¿åº¦æ˜¯å¦ä¸€è‡´"""
    if len(row['angledata']) != len(row['torquedata']):
        print(f"è­¦å‘Šï¼šæ ·æœ¬ {row.name} çš„angledataå’Œtorquedataé•¿åº¦ä¸ä¸€è‡´ï¼ˆ{len(row['angledata'])} vs {len(row['torquedata'])}ï¼‰ï¼Œå·²è·³è¿‡")
        return False
    return True


def filter_negative_values(angle_list, torque_list, min_value=0):
    """
    åŒæ­¥è¿‡æ»¤angleå’Œtorqueä¸­çš„è´Ÿå€¼æ•°æ®ï¼ˆä¿ç•™ä¸¤è€…å‡â‰¥min_valueçš„ç‚¹ï¼‰
    è¿”å›è¿‡æ»¤åçš„(angle_sublist, torque_sublist)
    """
    # ç”Ÿæˆç´¢å¼•åˆ—è¡¨ï¼Œä»…ä¿ç•™ä¸¤è€…å‡â‰¥min_valueçš„ä½ç½® 
    valid_indices = [
        idx for idx, (a, t) in enumerate(zip(angle_list, torque_list)) 
        if a > min_value and t > min_value
    ]
    # æå–è¿‡æ»¤åçš„å­åˆ—è¡¨ï¼ˆè‹¥æ²¡æœ‰æœ‰æ•ˆæ•°æ®è¿”å›ç©ºåˆ—è¡¨ï¼‰
    angle_sub = [angle_list[idx] for idx in valid_indices] if valid_indices else []
    torque_sub = [torque_list[idx] for idx in valid_indices] if valid_indices else []
    return angle_sub, torque_sub


def getValidGroups(df_use):
    valid_df = df_use[df_use.apply(validate_data, axis=1)]
    grouped = valid_df.groupby(['recommended_action', 'screwindex'])

    # ==================== ç»˜å›¾å‡†å¤‡ ====================
    valid_groups = [
        (action, s_idx) 
        for action in ['Continue', 'Rework', 'FA']  # æŒ‰æ¨èåŠ¨ä½œå›ºå®šé¡ºåº
        for s_idx in screw_order  # æŒ‰è‡ªå®šä¹‰èºä¸ç´¢å¼•é¡ºåº
        if (action, s_idx) in grouped.groups
    ]
    return valid_groups, grouped


# -----------------------------
# 3. æå–æ¯ä¸ªèºä¸æ‰“èºä¸è¿‡ç¨‹çš„ç‰¹å¾
# -----------------------------
def extract_features(row):
    raw_angle = row['angledata']
    raw_torque = row['torquedata']
    angle_v, torque_v = filter_negative_values(raw_angle, raw_torque, min_value=0)

    if len(angle_v) < 2:
        return pd.Series([np.nan]*12, index=['hsgsn', 'time', 'max_torque','angle_at_max','auc', 'slope_mid','slope_rise_late','total_angle', 'std_torque','row_span','valid', 'recommended_action'])

    # 1. æœ€å¤§æ‰­åŠ› & å¯¹åº”è§’åº¦
    max_torque = np.max(torque_v)
    angle_at_max = angle_v[np.argmax(torque_v)]

    # 2. âœ… AUCï¼šä»¥ angle ä¸ºæ¨ªåæ ‡çš„ç§¯åˆ† âˆ« torque d(angle)
    auc = np.trapz(torque_v, angle_v)  # å•ä½: NÂ·mÂ·Â°

    # æ–œç‡
    mid_angle = angle_v[len(angle_v) // 2]
    mid_torque = torque_v[len(angle_v) // 2]
    slope_mid = (max_torque - mid_torque) / (np.max(angle_v) - mid_angle + 1e-8)
    
    # ä¸Šå‡æ®µæ–œç‡ï¼šå‰50%ä¸Šå‡éƒ¨åˆ†
    peak_idx = np.argmax(torque_v)
    mid_torque = 0.5 * max_torque
    if peak_idx < 2:
        slope_rise_late = 0.0
    else:
        # æ‰¾å‡ºååŠä¸Šå‡æ®µçš„ç´¢å¼•
        late_rise_mask = (angle_v >= mid_torque) & (torque_v <= max_torque)
        late_rise_points = np.where(late_rise_mask)[0]

        if len(late_rise_points) >= 2:
            idx_start = late_rise_points[0]
            idx_end = late_rise_points[-1]
            delta_angle = angle_v[idx_end] - angle_v[idx_start]
            delta_torque = torque_v[idx_end] - torque_v[idx_start]
            slope_rise_late = delta_torque / (delta_angle + 1e-8)  # é˜²é™¤é›¶
        else:
            slope_rise_late = 0.0
    
    # 4. æ€»è§’åº¦è·¨åº¦ï¼ˆæœ‰æ•ˆæ®µï¼‰
    total_angle = angle_v[-1] - angle_v[0]
    # 5. æ‰­åŠ›æ ‡å‡†å·®
    std_torque = np.std(torque_v)
    # 7. row_in_file è·¨åº¦
    row_span = len(angle_v)

    return pd.Series([
        row['hsgsn'], row['time'], max_torque, angle_at_max, auc, slope_mid, slope_rise_late, total_angle, std_torque, row_span, True, row['recommended_action']
    ], index=[
        'hsgsn', 'time', 'max_torque','angle_at_max','auc', 'slope_mid','slope_rise_late','total_angle', 'std_torque','row_span','valid', 'recommended_action'
    ])


def process_location_data(df_use: pd.DataFrame):
    loc_groups = df_use['screwindex'].unique()
    all_features_dfs = []

    for loc in loc_groups:
        group_df = df_use[df_use['screwindex'] == loc].copy().sort_values('time')
        print(f"\nğŸ”§ æ­£åœ¨å¤„ç† loc={loc}ï¼Œå…± {len(group_df)} ä¸ªèºä¸")
        
        # æŒ‰ screw_id åˆ†ç»„æå–ç‰¹å¾
        features = group_df.apply(extract_features, axis=1)
        
        # æ£€æŸ¥å¹¶å¤„ç† NaN å€¼
        if features.isna().any(axis=None):  # å¦‚æœå­˜åœ¨ä»»ä½• NaN å€¼
            features['is_abnormal'] = features.isna().any(axis=1)  # æ ‡è®°åŒ…å« NaN çš„è¡Œä¸º True
            features['abnormal_reason'] = np.where(features['is_abnormal'], 'æ‰¹æ†é—®é¢˜(å¯èƒ½)', '')  # å‡è®¾ NaN æ˜¯ç”±æ‰¹æ†é—®é¢˜å¼•èµ·çš„
            
            # å¯¹äºæœ‰ NaN å€¼çš„è¡Œï¼Œä½ å¯ä»¥é€‰æ‹©å¡«å……ã€åˆ é™¤æˆ–è¿›ä¸€æ­¥å¤„ç†
            # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ç”¨é»˜è®¤å€¼å¡«å……ä»¥ä¾¿ç»§ç»­å¤„ç†
            features.fillna({'max_torque': -1, 'angle_at_max': -1, 'auc': -1,
                             'slope_rise_late': -1, 'total_angle': -1,
                             'std_torque': -1, 'duration_sec': -1, 'row_span': -1}, inplace=True)
        else:
            features['is_abnormal'] = False
            features['abnormal_reason'] = ''
        
        features = features[features['valid'].fillna(False)].reset_index(drop=True)
        # æ·»åŠ  loc æ ‡ç­¾
        features['loc'] = loc
        
        all_features_dfs.append(features)

    # åˆå¹¶æ‰€æœ‰ loc çš„ç‰¹å¾
    final_features = pd.concat(all_features_dfs, ignore_index=True)
    final_features = final_features.reset_index(drop=True)
    
    return final_features


def runProcess(df, window_size=10):
    final_features = process_location_data(df)
    loc_groups = final_features['loc'].unique()
    feature_cols = ['max_torque', 'angle_at_max', 'auc', 'slope_rise_late', 'total_angle', 'std_torque', 'row_span']
    bad_results = []

    for loc in loc_groups:
        data = final_features[final_features['loc'] == loc].copy()
        # æ»‘åŠ¨çª—å£æ ‡å‡†åŒ–
        for col in feature_cols:
            data[f'{col}_z'] = (data[col] - data[col].rolling(window=window_size, min_periods=1).mean()).abs() / (data[col].rolling(window=window_size, min_periods=1).std() + 1e-6)

        clf = IsolationForest(n_estimators=100, contamination=0.005, random_state=1)
        X = data[[f'{f}_z' for f in feature_cols]].fillna(0)
        # X = data[['max_torque', 'angle_at_max', 'auc', 'slope_rise_late', 'total_angle', 'std_torque', 'row_span']].fillna(0)
        # è®¡ç®—åŠ æƒå¼‚å¸¸åˆ†æ•°
        y_pred = clf.fit_predict(X)
        anomaly_scores = -clf.score_samples(X)  # è½¬æ¢ä¸º"è¶Šå¼‚å¸¸åˆ†æ•°è¶Šé«˜"
        data['anomaly_score'] = anomaly_scores
        data['is_anomaly'] = (y_pred == -1).astype(int)
        bad_results.append(data[(data['recommended_action'] != 'Continue') | (data['is_anomaly'] == 1)])

    bad_results = pd.concat(bad_results, ignore_index=True)
    bad_results[['hsgsn', 'time', 'recommended_action', 'is_anomaly', 'loc', 'anomaly_score']]
    return bad_results